version: '3'
services:
  # This service runs once to initialize the Airflow database
  airflow-init:
    build: .
    command: version # A simple command to trigger the build and db init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW_ADMIN_USER=admin
      - AIRFLOW_ADMIN_PASSWORD=admin
      - AIRFLOW_ADMIN_FIRSTNAME=Admin
      - AIRFLOW_ADMIN_LASTNAME=User
      - AIRFLOW_ADMIN_EMAIL=admin@example.com
    depends_on:
      postgres:
        condition: service_healthy

  # The main Airflow webserver (UI)
  webserver:
    build: .
    command: webserver
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      # This is crucial: it tells Airflow where our dbt project is inside the container
      - DBT_PROJECT_DIR=/opt/airflow/dbt
      
    volumes:
      # Mount our local folders to the correct paths inside the container
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
      - ./ml_scripts:/opt/airflow/ml_scripts
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # The scheduler that runs your DAGs
  scheduler:
    build: .
    command: scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - DBT_PROJECT_DIR=/opt/airflow/dbt
     
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
      - ./ml_scripts:/opt/airflow/ml_scripts

  # The database for Airflow itself
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
      
  ml-trainer:
    build:
      context: .
      dockerfile: Dockerfile.ml # Hum apna naya ML Dockerfile use karenge
    # Yeh container hamesha nahi chalega, Airflow isse jab zaroorat hogi tab start karega
    profiles: ["donotstart"] 
    volumes:
      # Scripts tak access
      - ./ml_scripts:/app/ml_scripts
      # Trained model save karne ke liye data folder tak access
      - ./data:/app/data
  recommender-api:
    build:
      context: ./api
      dockerfile: Dockerfile
    # API hamesha chalni chahiye
    restart: always
    volumes:
      # API ke code tak access
      - ./api:/app
      # Model files tak access (Trained model yahan hai)
      - ./data:/app/data
    ports:
      # Local port 8001 ko container ke port 8000 se jod rahe hain
      - "8001:8000"
    # Isse api ko dbt/Airflow ke network mein jaane ki zaroorat nahi hai
    network_mode: bridge

volumes:
  postgres-db-volume:

